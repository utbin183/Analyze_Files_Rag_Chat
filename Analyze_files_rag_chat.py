import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import TokenTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import UnstructuredWordDocumentLoader
import json
import os
import tempfile
import hashlib
import pytesseract
from PIL import Image
import io
import fitz
import google.generativeai as genai
from openai import OpenAI
from openai import OpenAIError
from docx import Document
from dotenv import load_dotenv
import base64
import pandas as pd
from pathlib import Path
import uuid
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.dates as mdates


# Load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

# L·∫•y API key t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
google_api_key = os.getenv("GOOGLE_API_KEY")
openai_api_key = os.getenv("OPENAI_API_KEY")

# N·∫øu kh√¥ng c√≥ c·∫£ Google API key v√† OpenAI API key th√¨ b√°o l·ªói v√† d·ª´ng streamlit
if not google_api_key and not openai_api_key:
    st.error("Kh√¥ng t√¨m th·∫•y API key n√†o (Google ho·∫∑c OpenAI)")
    st.stop()

# N·∫øu c√≥ Google API key th√¨ c·∫•u h√¨nh cho th∆∞ vi·ªán google.generativeai
if google_api_key:
    genai.configure(api_key=google_api_key)
    

# ƒê·ªçc file JSON
def load_language_strings(file_path="language_strings.json"):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        st.error(f"Kh√¥ng th·ªÉ ƒë·ªçc file language_strings.json: {e}")
        return {}  # Tr·∫£ v·ªÅ t·ª´ ƒëi·ªÉn r·ªóng n·∫øu l·ªói

# T·∫£i t·ª´ ƒëi·ªÉn ng√¥n ng·ªØ
LANGUAGE_STRINGS = load_language_strings()
if not LANGUAGE_STRINGS:
    st.stop()  # D·ª´ng ·ª©ng d·ª•ng n·∫øu kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file JSON

# Kh·ªüi t·∫°o tr·∫°ng th√°i ng√¥n ng·ªØ
if "language" not in st.session_state:
    st.session_state["language"] = "vi"  # Ng√¥n ng·ªØ m·∫∑c ƒë·ªãnh l√† ti·∫øng Vi·ªát
    
def save_bytes_to_tempfile(file_bytes, suffix):
    """Ghi bytes v√†o file t·∫°m v√† tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n."""
    # T·∫°o 1 file t·∫°m (temporary file) v·ªõi ph·∫ßn ƒëu√¥i (suffix) do ng∆∞·ªùi d√πng truy·ªÅn v√†o
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
    # Ghi d·ªØ li·ªáu d·∫°ng bytes v√†o file t·∫°m
    tmp.write(file_bytes)
    # ƒê·∫£m b·∫£o d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c ghi h·∫øt xu·ªëng ·ªï ƒëƒ©a
    tmp.flush()
    # ƒê√≥ng file
    tmp.close()
    # Tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n file t·∫°m ƒë·ªÉ c√≥ th·ªÉ m·ªü l·∫°i sau
    return tmp.name

def image_bytes_ocr(img_bytes):
    try:
        # Chuy·ªÉn bytes th√†nh ƒë·ªëi t∆∞·ª£ng ·∫£nh b·∫±ng Pillow
        img = Image.open(io.BytesIO(img_bytes))
        # D√πng pytesseract OCR nh·∫≠n di·ªán ch·ªØ (c·∫£ ti·∫øng Anh + ti·∫øng Vi·ªát)
        text = pytesseract.image_to_string(img, lang='eng+vie')
        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng ƒë·∫ßu/cu·ªëi v√† tr·∫£ v·ªÅ
        return text.strip()
    except Exception as e:
        # N·∫øu c√≥ l·ªói (·∫£nh h·ªèng, OCR l·ªói...) th√¨ tr·∫£ v·ªÅ chu·ªói r·ªóng
        return ""

def clear_chat_history():
    # X√≥a l·ªãch s·ª≠ chat trong session state c·ªßa Streamlit.
    st.session_state["chat_history"] = []

def get_text_from_file(file_bytes, filename):
    # L·∫•y ƒë·ªãnh d·∫°ng file
    ext = Path(filename).suffix.lower()
    tmp_path = None
    try:
        # ==================X·ª≠ l√Ω PDF ==================
        if ext in [".pdf"]:
            # L∆∞u file PDF ra file t·∫°m
            tmp_path = save_bytes_to_tempfile(file_bytes, suffix=".pdf")
            
            # D√πng PyPDFLoader ƒë·ªÉ ƒë·ªçc vƒÉn b·∫£n trong PDF, chia th√†nh nhi·ªÅu trang
            loader = PyPDFLoader(tmp_path)
            pages = loader.load_and_split()
            
            # Gh√©p n·ªôi dung c√°c trang th√†nh 1 chu·ªói text
            text = "\n".join(p.page_content for p in pages)
            
            # Preview m·∫∑c ƒë·ªãnh cho PDF (s·∫Ω hi·ªÉn th·ªã nh√∫ng PDF sau)
            preview = None  
            images = []  # ch·ª©a c√°c ·∫£nh tr√≠ch xu·∫•t t·ª´ PDF
            ocr_texts = [] # ch·ª©a vƒÉn b·∫£n OCR ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ ·∫£nh
            seen_hashes = set() # ƒë·ªÉ tr√°nh x·ª≠ l√Ω tr√πng ·∫£nh
            
            # M·ªü PDF b·∫±ng PyMuPDF (fitz) ƒë·ªÉ tr√≠ch xu·∫•t ·∫£nh
            pdf_doc = fitz.open(tmp_path)
            for page in pdf_doc:
                for img in page.get_images(full=True):
                    xref = img[0] # ID ·∫£nh
                    base_image = pdf_doc.extract_image(xref)
                    img_bytes = base_image["image"]
                    img_format = base_image["ext"]
                    
                    # T√≠nh hash ƒë·ªÉ tr√°nh duplicate ·∫£nh
                    img_hash = hashlib.md5(img_bytes).hexdigest()
                    if img_hash not in seen_hashes:
                        seen_hashes.add(img_hash)
                        
                        # Chuy·ªÉn ·∫£nh th√†nh base64 ƒë·ªÉ hi·ªÉn th·ªã trong Streamlit
                        b64_img = base64.b64encode(img_bytes).decode("utf-8")
                        images.append({"format": img_format, "b64": b64_img})
                        
                        # OCR vƒÉn b·∫£n trong ·∫£nh 
                        ocr_txt = image_bytes_ocr(img_bytes)
                        if ocr_txt:
                            ocr_texts.append(ocr_txt)
            # N·∫øu c√≥ vƒÉn b·∫£n OCR t·ª´ ·∫£nh th√¨ n·ªëi th√™m v√†o text
            if ocr_texts:
                text += "\n\n[Text tr√≠ch xu·∫•t t·ª´ ·∫£nh]\n" + "\n".join(ocr_texts)
            # T·∫°o preview cho PDF (hi·ªÉn th·ªã text + ·∫£nh tr√≠ch xu·∫•t)
            preview = {"type": "pdf", "text_preview": text[:1000], "images": images}
            return text, preview

        # ==================== X·ª¨ L√ù DOCX ====================
        elif ext in [".docx"]:
            # L∆∞u file DOCX ra file t·∫°m
            tmp_path = save_bytes_to_tempfile(file_bytes, suffix=".docx")
            
            # D√πng Unstructured loader ƒë·ªÉ ƒë·ªçc vƒÉn b·∫£n
            loader = UnstructuredWordDocumentLoader(tmp_path)
            pages = loader.load_and_split()
            text = "\n".join(p.page_content for p in pages)
            
            # ƒê·ªçc file docx b·∫±ng python-docx ƒë·ªÉ tr√≠ch xu·∫•t ·∫£nh
            doc = Document(tmp_path)
            images = []
            ocr_texts = []
            seen_hashes = set()
            for rel in doc.part.rels.values():
                if "image" in rel.target_ref:
                    img_bytes = rel.target_part.blob
                    img_hash = hashlib.md5(img_bytes).hexdigest()
                    if img_hash not in seen_hashes:
                        seen_hashes.add(img_hash)
                        
                        # L∆∞u ·∫£nh d·∫°ng base64
                        b64_img = base64.b64encode(img_bytes).decode("utf-8")
                        images.append({"format": "png", "b64": b64_img})
                        
                        # OCR text trong ·∫£nh (n·∫øu c√≥)
                        ocr_txt = image_bytes_ocr(img_bytes)
                        if ocr_txt:
                            ocr_texts.append(ocr_txt)
                            
            # N·∫øu OCR ƒë∆∞·ª£c text t·ª´ ·∫£nh th√¨ n·ªëi v√†o vƒÉn b·∫£n
            if ocr_texts:
                text += "\n\n[Text tr√≠ch xu·∫•t t·ª´ ·∫£nh]\n" + "\n".join(ocr_texts)
            # T·∫°o preview cho DOCX
            preview = {"type": "docx", "text_preview": text[:30000000], "images": images}
            return text, preview
        
        # ==================== X·ª¨ L√ù EXCEL & CSV ====================
        elif ext in [".xlsx", ".csv"]:
            tmp_path = save_bytes_to_tempfile(file_bytes, suffix=ext)
            
            # ƒê·ªçc d·ªØ li·ªáu b·∫£ng
            if ext == ".csv":
                df = pd.read_csv(tmp_path)
                sheets = {"Sheet1": df} # CSV coi nh∆∞ 1 sheet
            else:
                # ƒê·ªçc t·∫•t c·∫£ sheet trong Excel
                sheets = pd.read_excel(tmp_path, sheet_name=None, engine="openpyxl")
            text_parts = []
            preview = {"type": "xlsx", "sheets": sheets}
            
            # Convert d·ªØ li·ªáu m·ªói sheet sang d·∫°ng text (gi·ªëng CSV)
            for sheet_name, df in sheets.items():
                csv_like = df.to_csv(index=False)
                text_parts.append(f"Sheet: {sheet_name}\n{csv_like}")
                
            # Gh√©p to√†n b·ªô sheet th√†nh text
            text = "\n\n".join(text_parts)
            return text, preview
        # ==================== ƒê·ªäNH D·∫†NG KH√îNG H·ªñ TR·ª¢ ====================
        else:
            return "", {"type": "unknown", "msg": f"ƒê·ªãnh d·∫°ng {ext} ch∆∞a ƒë∆∞·ª£c h·ªó tr·ª£"}
    
    except Exception as e:
        # B√°o l·ªói ra Streamlit n·∫øu c√≥ s·ª± c·ªë khi ƒë·ªçc file
        st.error(f"L·ªói ƒë·ªçc file {filename}: {e}")
        return "", {"type": "error", "msg": str(e)}
    finally:
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.remove(tmp_path)
            except Exception:
                pass

def make_df_arrow_compatible(df):
    """Chuy·ªÉn ƒë·ªïi DataFrame ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi Arrow serialization"""
    for col in df.columns:
        dtype = df[col].dtype
        
        # N·∫øu c·ªôt d√πng pandas extension dtypes (vd: Int64, StringDtype, BooleanDtype)
        if pd.api.types.is_extension_array_dtype(dtype):
            
            # N·∫øu l√† s·ªë nguy√™n (nullable int)
            if pd.api.types.is_integer_dtype(dtype):
                if df[col].isna().any():
                    # N·∫øu c√≥ NaN th√¨ kh√¥ng th·ªÉ gi·ªØ int => chuy·ªÉn th√†nh float64
                    df[col] = df[col].astype('float64')
                else:
                    # N·∫øu kh√¥ng c√≥ NaN th√¨ gi·ªØ int64 chu·∫©n
                    df[col] = df[col].astype('int64')
            
            # N·∫øu l√† s·ªë th·ª±c (float dtype nh∆∞ng ·ªü d·∫°ng extension)
            elif pd.api.types.is_float_dtype(dtype):
                df[col] = df[col].astype('float64')
            
            # N·∫øu l√† chu·ªói (StringDtype)
            elif pd.api.types.is_string_dtype(dtype):
                # √âp v·ªÅ object (chu·∫©n h∆°n cho Arrow)
                df[col] = df[col].astype('object')
            
            # C√°c lo·∫°i extension kh√°c (vd: category, boolean nullable)
            else:
                # Chuy·ªÉn v·ªÅ object ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng l·ªói
                df[col] = df[col].astype('object')
        
        # N·∫øu l√† object dtype (c√≥ th·ªÉ ch·ª©a nhi·ªÅu lo·∫°i d·ªØ li·ªáu)
        if dtype == 'object':
            pass
    return df

def show_preview_from_file(file_bytes, filename, preview_info):
    lang = st.session_state["language"]
    if preview_info is None:
        st.info(LANGUAGE_STRINGS[lang]["no_preview"])
        return
    
    # ====== Preview cho PDF ======
    if preview_info.get("type") == "pdf":
        st.subheader(LANGUAGE_STRINGS[lang]["pdf_preview"].format(filename=filename))
        
        # M√£ h√≥a PDF th√†nh base64 r·ªìi nh√∫ng v√†o iframe ƒë·ªÉ hi·ªÉn th·ªã trong Streamlit
        base64_pdf = base64.b64encode(file_bytes).decode("utf-8")
        pdf_display = f'<iframe src="data:application/pdf;base64,{base64_pdf}" width="100%" height="600" type="application/pdf"></iframe>'
        st.markdown(pdf_display, unsafe_allow_html=True)
        
        # Hi·ªÉn th·ªã text tr√≠ch xu·∫•t t·ª´ PDF (preview ng·∫Øn)
        st.text(preview_info.get("text_preview", ""))
        
        # N·∫øu PDF c√≥ ch·ª©a ·∫£nh th√¨ hi·ªÉn th·ªã th√™m
        images = preview_info.get("images", [])
        if images:
            st.write("·∫¢nh trong file:" if lang == "vi" else "Images in file:")
            for img in images:
                st.image(f"data:image/{img['format']};base64,{img['b64']}", width="stretch")
    
    # ====== Preview cho DOCX ======
    elif preview_info.get("type") == "docx":
        st.subheader(f"Preview - {filename} (docx)")
        st.text(preview_info.get("text_preview", ""))
        
        # Hi·ªÉn th·ªã ·∫£nh trong file Word (n·∫øu c√≥)
        images = preview_info.get("images", [])  
        if images:
            st.write("·∫¢nh trong file:" if lang == "vi" else "Images in file:")
            for img in images:
                st.image(f"data:image/{img['format']};base64,{img['b64']}", width="stretch")
    
    # ====== Preview cho Excel/CSV ======
    elif preview_info.get("type") == "xlsx":
        st.subheader(LANGUAGE_STRINGS[lang]["xlsx_preview"].format(filename=filename))
        sheets = preview_info["sheets"]
        
        # Cho ph√©p ch·ªçn sheet ƒë·ªÉ hi·ªÉn th·ªã
        selected_sheet = st.selectbox(
            LANGUAGE_STRINGS[lang]["sheet_select"].format(filename=filename),
            list(sheets.keys()),
            key=f"select_sheet_{hash(filename)}"
        )
        if selected_sheet:
            df = sheets[selected_sheet]
            df = make_df_arrow_compatible(df)  # S·ª≠a l·ªói Arrow serialization
            st.write(f"Sheet: {selected_sheet}")
            st.dataframe(df.head(10))

            if df is not None:
                # ƒê∆∞a ra b·∫£ng th·ªëng k√™ m√¥ t·∫£ nhanh (t·ª± ƒë·ªông t√≠nh count, mean, std, min, 25%, 50%, 75%, max)
                st.subheader(LANGUAGE_STRINGS[lang]["stats_header"])
                st.write(df.describe())
                
                # Hi·ªÉn th·ªã s·ªë d√≤ng v√† s·ªë c·ªôt c·ªßa data
                st.subheader(LANGUAGE_STRINGS[lang]["shape_header"])
                st.write(df.shape)
                
                # Hi·ªÉn th·ªã b·∫£ng c√°c lo·∫°i ki·ªÉu d·ªØ li·ªáu c·ªßa t·ª´ng c·ªôt
                st.subheader(LANGUAGE_STRINGS[lang]["dtypes_header"])
                st.write(df.dtypes.astype(str))  
                
                # Hi·ªÉn th·ªã s·ªë l∆∞·ª£ng null data
                st.subheader(LANGUAGE_STRINGS[lang]["null_header"])
                st.write(df.isnull().sum())
                
                # Hi·ªÉn th·ªã s·ªë d√≤ng b·ªã tr√πng l·∫∑p
                st.subheader(LANGUAGE_STRINGS[lang]["duplicates_header"])
                st.write(df.duplicated().sum())
            
            st.header(LANGUAGE_STRINGS[lang]["data_handling_header"])
            # ƒê·ªïi ki·ªÉu d·ªØ li·ªáu n·∫øu ƒëang ·ªü d·∫°ng object (TH ƒë·∫∑c bi·ªát n·∫øu l√† ng√†y th√°ng nƒÉm th√¨ ƒë·ªïi th√†nh datetime64[ns]. C√≤n l·∫°i th√¨ chuy·ªÉn th√†nh ki·ªÉu d·ªØ li·ªáu ph√π h·ª£p)
            st.subheader(LANGUAGE_STRINGS[lang]["dtype_change_header"])
            for col in df.columns:
                current_dtype = df[col].dtype
                if current_dtype == "object" :
                    try:
                        df[col] = pd.to_datetime(df[col], errors="raise", dayfirst=True)
                    except Exception:
                        pass
            st.write(df.dtypes.astype(str))  
            
             # --- X·ª≠ l√Ω d·ªØ li·ªáu tr√πng l·∫∑p ---
            st.subheader(LANGUAGE_STRINGS[lang]["duplicates_handling_header"])
            # T·∫°o ra s·ª± l·ª±a ch·ªçn kh√¥ng l√†m ho·∫∑c x·ª≠ l√Ω d·ªØ li·ªáu b·ªã tr√πng
            cleaned_duplicated_data_actions = {
                LANGUAGE_STRINGS[lang]["duplicates_options"]["do_not_do_anything"]: "do_not_do_anything",
                LANGUAGE_STRINGS[lang]["duplicates_options"]["dropna_the_duplicated_rows"]: "dropna_the_duplicated_rows"
            }
            # S·ª≠ d·ª•ng b·∫£ng bƒÉm (hash) ƒë·ªÉ t·∫°o m·ªôt gi√° tr·ªã duy nh·∫•t (unique key) cho m·ªói filename 
            key_id = hash(filename + selected_sheet)
            selected_label = st.radio(
                "Duplicated Handling Strategy", 
                list(cleaned_duplicated_data_actions.keys()), 
                label_visibility="collapsed",
                key=f"duplicated_strategy_{key_id}"
            )
            
            cleaned_duplicated_data_action = cleaned_duplicated_data_actions[selected_label]
            # Kh√¥ng x·ª≠ l√Ω d·ªØ li·ªáu tr√πng
            if cleaned_duplicated_data_action == "do_not_do_anything":
                df = df 
            # X·ª≠ l√Ω d·ªØ li·ªáu b·ªã tr√πng b·∫±ng c√°ch x√≥a d√≤ng tr√πng
            elif cleaned_duplicated_data_action == "dropna_the_duplicated_rows":
                df = df.drop_duplicates()
            
            
            if df is not None: 
            # Hi·ªÉn th·ªã b·∫£ng dataframe sau khi x·ª≠ l√Ω d·ªØ li·ªáu b·ªã tr√πng 
                df = make_df_arrow_compatible(df)  # Fix Arrow issues
                st.subheader("üìä D·ªØ li·ªáu sau khi x·ª≠ l√Ω tr√πng l·∫∑p:" if lang == "vi" else "üìä The handled duplicated data: ")
                st.dataframe(df)
            # Hi·ªÉn th·ªã s·ªë l∆∞·ª£ng d√≤ng sau khi d·ªçn s·∫°ch
                st.subheader(LANGUAGE_STRINGS[lang]["duplicates_header"] + (" sau khi x·ª≠ l√Ω" if lang == "vi" else " after cleaned"))
                st.write(df.duplicated().sum())
            
            st.subheader(LANGUAGE_STRINGS[lang]["null_handling_header"])
            # T·∫°o ra c√°c l·ª±a ch·ªçn ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu b·ªã r·ªóng (null)
            cleaned_null_data_actions = {
                LANGUAGE_STRINGS[lang]["null_options"]["none"]: "none",
                LANGUAGE_STRINGS[lang]["null_options"]["dropna"]: "dropna",
                LANGUAGE_STRINGS[lang]["null_options"]["unknown"]: "unknown",
                LANGUAGE_STRINGS[lang]["null_options"]["ffill"]: "ffill",
                LANGUAGE_STRINGS[lang]["null_options"]["bfill"]: "bfill"
            }
            # S·ª≠ d·ª•ng b·∫£ng bƒÉm (hash) ƒë·ªÉ t·∫°o m·ªôt gi√° tr·ªã duy nh·∫•t (unique key) cho m·ªói filename 
            key_id = hash(filename + selected_sheet)
            selected_label = st.radio(
                "Null Handling Strategy",
                list(cleaned_null_data_actions.keys()),
                label_visibility="collapsed",
                key=f"null_strategy_{key_id}"
            )
            clean_null_data_action = cleaned_null_data_actions[selected_label]
            df_cleaned = df.copy()
            
            # Kh√¥ng x·ª≠ l√Ω d·ªØ li·ªáu b·ªã r·ªóng
            if clean_null_data_action == "none":
                df_cleaned = df_cleaned
            
            # X·ª≠ l√Ω d·ªØ li·ªáu b·ªã r·ªóng b·∫±ng c√°ch x√≥a d√≤ng b·ªã null
            elif clean_null_data_action == "dropna":
                df_cleaned = df_cleaned.dropna()
            
            # X·ª≠ l√Ω d·ªØ li·ªáu b·ªã r·ªóng b·∫±ng c√°ch thay b·∫±ng unknown (n·∫øu l√† object, string, datetime64[ns]) ho·∫∑c 0 (n·∫øu d·ªØ li·ªáu l√† ki·ªÉu c√≥ s·ªë nguy√™n ho·∫∑c s·ªë th·ª±c
            elif clean_null_data_action == "unknown":
                object_cols_string = df_cleaned.select_dtypes(include=["object", "string", "datetime64[ns]"]).columns
                object_cols_int_or_float = df_cleaned.select_dtypes(include=["int", "float", "int64", "float64", "Int64", "Float64"]).columns.to_list()
                existing_object_cols = [col for col in object_cols_string if col in df_cleaned.columns]
                if not object_cols_string.empty:
                    df_cleaned[existing_object_cols] = df_cleaned[existing_object_cols].fillna("Unknown")
                    st.info(f"Replaced nulls with 'Unknown' in columns: {', '.join(existing_object_cols)}")
                if object_cols_int_or_float:
                    for col in object_cols_int_or_float:
                        try:
                            df_cleaned[col] = df_cleaned[col].fillna(0)
                        except Exception as e:
                            st.warning(f"‚ö†Ô∏è Could not fill nulls in column `{col}`: {e}")
                    st.info(f"Replaced nulls with 0 in numeric columns: {', '.join(object_cols_int_or_float)}")
            
            # X·ª≠ l√Ω d·ªØ li·ªáu b·ªã null b·∫±ng s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p ffill 
            # Ph∆∞∆°ng ph√°p ffill l√† forward fill t·ª©c l√† ƒëi·ªÅn gi√° tr·ªã b·ªã thi·∫øu (NaN) b·∫±ng gi√° tr·ªã g·∫ßn nh·∫•t ph√≠a tr∆∞·ªõc n√≥ (tr√™n c√πng m·ªôt c·ªôt).
            elif clean_null_data_action == "ffill":
                df_cleaned = df_cleaned.fillna(method="ffill")
            
            # X·ª≠ l√Ω d·ªØ li·ªáu b·ªã null b·∫±ng s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p bfill 
            # Ph∆∞∆°ng ph√°p bfill l√† backward fill t·ª©c l√† ƒëi·ªÅn gi√° tr·ªã b·ªã thi·∫øu (NaN) b·∫±ng gi√° tr·ªã g·∫ßn nh·∫•t ph√≠a sau n√≥ (tr√™n c√πng m·ªôt c·ªôt).
            elif clean_null_data_action == "bfill":
                df_cleaned = df_cleaned.fillna(method="bfill")
            
            
            if df_cleaned is not None: 
                # Hi·ªÉn th·ªã b·∫£ng dataframe sau khi x·ª≠ l√Ω d·ªØ li·ªáu b·ªã r·ªóng
                df_cleaned = make_df_arrow_compatible(df_cleaned) 
                st.subheader("üìä D·ªØ li·ªáu sau khi x·ª≠ l√Ω null:" if lang == "vi" else "üìä The handled null data:")
                st.dataframe(df_cleaned)
                # Hi·ªÉn th·ªã s·ªë l∆∞·ª£ng d√≤ng sau khi d·ªçn s·∫°ch d·ªØ li·ªáu r·ªóng
                st.subheader(LANGUAGE_STRINGS[lang]["null_header"] + (" sau khi x·ª≠ l√Ω" if lang == "vi" else " after cleaned "))
                st.write(df_cleaned.isnull().sum())
                
            # T·∫°o n√∫t download csv sau khi x·ª≠ l√Ω d·ªØ li·ªáu
            st.subheader(LANGUAGE_STRINGS[lang]["download_button"])
            csv = df_cleaned.to_csv(index=False).encode('utf-8')
            st.download_button(
                LANGUAGE_STRINGS[lang]["download_button"],
                csv,
                file_name=LANGUAGE_STRINGS[lang]["download_filename"].format(filename=filename, sheet=selected_sheet),
                mime="text/csv",
                key=f"download_{key_id}"
            )
            st.subheader(LANGUAGE_STRINGS[lang]["chart_header"])
            
            # X√°c ƒë·ªãnh c√°c lo·∫°i c·ªôt trong dataframe
            categorical_cols = df_cleaned.select_dtypes(include=["object", "string", "category"]).columns.tolist()
            numerical_cols = df_cleaned.select_dtypes(include=["int64", "float64"]).columns.tolist()
            datetime_cols = df_cleaned.select_dtypes(include=["datetime64"]).columns.tolist()
            
            # Select box ƒë·ªÉ ch·ªçn lo·∫°i bi·ªÉu ƒë·ªì
            chart_type = st.selectbox(
                LANGUAGE_STRINGS[lang]["chart_type_label"],
                LANGUAGE_STRINGS[lang]["chart_type_options"],
                key=f"chart_type_{key_id}"
            )
            
            # Bi·ªÉu ƒë·ªì tr√≤n
            if chart_type == "pie":
                if categorical_cols:
                    selected_cat = st.selectbox(
                        LANGUAGE_STRINGS[lang]["pie_col_label"],
                        categorical_cols,
                        key=f"pie_cat_{key_id}"
                    )
                    value_counts = df_cleaned[selected_cat].value_counts().head(10)
                    fig, ax = plt.subplots(figsize=(6, 6))
                    ax.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%', startangle=90, colors=sns.color_palette("pastel"))
                    ax.set_title(f"Ph√¢n b·ªë c·ªßa '{selected_cat}' (Top 10)" if lang == "vi" else f"The distribution of '{selected_cat}' (Top 10)")
                    st.pyplot(fig)
                else:
                    st.info("Don't have column to draw the pie chart")
            
            # Bi·ªÉu ƒë·ªì c·ªôt
            elif chart_type == "bar":
                if categorical_cols and numerical_cols:
                    selected_cat = st.selectbox(
                        LANGUAGE_STRINGS[lang]["bar_cat_label"],
                        categorical_cols,
                        key=f"bar_cat_{key_id}"
                    )
                    selected_num = st.selectbox(
                        LANGUAGE_STRINGS[lang]["bar_num_label"],
                        numerical_cols,
                        key=f"bar_num_{key_id}"
                    )
                    agg_func = st.selectbox(
                        LANGUAGE_STRINGS[lang]["bar_agg_label"],
                        ["mean", "sum", "count", "min", "max"],
                        key=f"bar_agg_{key_id}"
                    )
                    agg_df = df_cleaned.groupby(selected_cat)[selected_num].agg(agg_func).sort_values(ascending=False).reset_index()
            
                    fig, ax = plt.subplots(figsize=(12, 6))
                    sns.barplot(x=selected_cat, y=selected_num, data=agg_df, ax=ax, palette="pastel")
                    for index, row in agg_df.iterrows():
                        ax.text(index, row[selected_num], f"{row[selected_num]:.2f}", ha='center', va='bottom', fontsize=8)
                    ax.set_title(f"{agg_func.capitalize()} c·ªßa {selected_num} theo {selected_cat}" if lang == "vi" else f"{agg_func.capitalize()} of {selected_num} by {selected_cat}")
                    ax.set_xlabel(selected_cat)
                    ax.set_ylabel(f"{agg_func.capitalize()} {selected_num}")
                    ax.tick_params(axis='x', rotation=45)
                    ax.grid(axis='y', linestyle='--', alpha=0.7)
                    st.pyplot(fig)
                else:
                    st.info(LANGUAGE_STRINGS[lang]["no_bar_cols"])
            
            # Bi·ªÉu ƒë·ªì ƒë∆∞·ªùng
            elif chart_type == "line":
                x_cols = numerical_cols + datetime_cols  # Tr·ª•c x c√≥ th·ªÉ l√† s·ªë ho·∫∑c ng√†y gi·ªù
                if x_cols and numerical_cols:
                    selected_x = st.selectbox(
                        LANGUAGE_STRINGS[lang]["line_x_label"],
                        x_cols,
                        key=f"line_x_{key_id}"
                    )
                    selected_y = st.selectbox(
                        LANGUAGE_STRINGS[lang]["line_y_label"],
                        numerical_cols,
                        key=f"line_y_{key_id}"
                    )
                    if selected_x and selected_y:
                        fig, ax = plt.subplots(figsize=(12, 6))
                        sorted_df = df_cleaned.sort_values(by=selected_x)  # S·∫Øp x·∫øp theo tr·ª•c x
                        ax.plot(sorted_df[selected_x], sorted_df[selected_y], marker='o', linestyle='-', color='b')
                        ax.set_title(f"{selected_y} theo {selected_x}" if lang == "vi" else f"{selected_y} by {selected_x}" )
                        ax.set_xlabel(selected_x)
                        ax.set_ylabel(selected_y)
                        if pd.api.types.is_datetime64_any_dtype(df_cleaned[selected_x]):  # X·ª≠ l√Ω tr·ª•c x n·∫øu l√† ng√†y gi·ªù
                            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
                            ax.xaxis.set_major_locator(mdates.AutoDateLocator())
                            plt.xticks(rotation=45)
                        ax.grid(True)
                        st.pyplot(fig)
                    else:
                        st.info("Vui l√≤ng ch·ªçn c·ªôt cho tr·ª•c ngang v√† tr·ª•c d·ªçc" if lang == "vi" else "Please choose column for horizontal and vertical" )
                else:
                    st.info(LANGUAGE_STRINGS[lang]["no_line_cols"])
            
def get_text_chunks(text, chunk_size, chunk_overlap):
    try:
        # D√πng TokenTextSplitter ƒë·ªÉ c·∫Øt vƒÉn b·∫£n th√†nh nhi·ªÅu ph·∫ßn nh·ªè
        splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        return splitter.split_text(text)
    except Exception as e:
        # B√°o l·ªói n·∫øu qu√° tr√¨nh chia chunk th·∫•t b·∫°i
        st.error(f"L·ªói chia chunk: {e}" if lang == "vi" else f"Error splitting chunks: {e}")
        return []

def get_vector_store(text_chunks,session_dir, provider="Gemini"):
    try:
         # Ch·ªçn model embedding theo provider
        if provider == "Gemini":
            embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        else:  
            embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
         # T·∫°o FAISS vector database t·ª´ c√°c text_chunks
        vector_store = FAISS.from_texts(text_chunks, embeddings)
         # L∆∞u vector database v√†o th∆∞ m·ª•c session_dir
        vector_store.save_local(session_dir)
        # Th√¥ng b√°o th√†nh c√¥ng
        st.success("T√†i li·ªáu ƒë√£ ƒë∆∞·ª£c ph√¢n t√≠ch xong, s·∫µn s√†ng tr·∫£ l·ªùi" if lang == "vi" else "Document analysis is done, ready to answer")
    except OpenAIError as e:
        if e.code == "insufficient_quota":
            st.error("ƒê√£ v∆∞·ª£t qu√° quota OpenAI. Vui l√≤ng ki·ªÉm tra g√≥i d·ªãch v·ª• ho·∫∑c chuy·ªÉn sang Gemini." if lang == "vi" else "OpenAI quota exceeded. Please check your service plan or switch to Gemini.")
        else:
            st.error(f"L·ªói l∆∞u vector database: {e}" if lang == "vi" else f"Error saving vector database: {e}")
    except Exception as e:
        # B√°o l·ªói n·∫øu qu√° tr√¨nh t·∫°o/l∆∞u vector store th·∫•t b·∫°i
        st.error(f"L·ªói l∆∞u vector database: {e}" if lang == "vi" else f"Error saving vector database: {e}")

def get_conversational_chain(answer_mode="Chi ti·∫øt", show_reasoning=False, provider="Gemini"):
    # X√°c ƒë·ªãnh phong c√°ch tr·∫£ l·ªùi
    style = "ng·∫Øn g·ªçn, s√∫c t√≠ch" if answer_mode == "Ng·∫Øn g·ªçn" else "chi ti·∫øt, ƒë·∫ßy ƒë·ªß" 

    # N·∫øu ng∆∞·ªùi d√πng mu·ªën th·∫•y reasoning steps th√¨ th√™m y√™u c·∫ßu v√†o prompt
    reasoning_part = ""
    if show_reasoning:
        reasoning_part = "\nNgo√†i ra, h√£y ƒë∆∞a ra m·ªôt ƒëo·∫°n 'T√≥m t·∫Øt reasoning steps' gi·∫£i th√≠ch ng·∫Øn g·ªçn c√°ch b·∫°n suy lu·∫≠n t·ª´ ng·ªØ c·∫£nh.\n"
    
    # Prompt template: r√†ng bu·ªôc model tr·∫£ l·ªùi CH√çNH X√ÅC theo ng·ªØ c·∫£nh
    prompt_template = f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI, nhi·ªám v·ª• l√† tr·∫£ l·ªùi d·ª±a *ch√≠nh x√°c* v√†o ng·ªØ c·∫£nh cung c·∫•p. 
Tuy·ªát ƒë·ªëi **kh√¥ng b·ªãa** n·∫øu ng·ªØ c·∫£nh kh√¥ng c√≥ th√¥ng tin. 
N·∫øu kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi, h√£y tr·∫£ l·ªùi ƒë√∫ng nguy√™n vƒÉn: "C√¢u tr·∫£ l·ªùi kh√¥ng c√≥ trong ng·ªØ c·∫£nh."

Y√™u c·∫ßu: Tr·∫£ l·ªùi theo phong c√°ch {style}.
{reasoning_part}

Ng·ªØ c·∫£nh: {{context}}
C√¢u h·ªèi: {{question}}

Answer:
"""

    try:
         # Ch·ªçn model theo provider
        if provider == "Gemini": #Gemini
            model = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.1)
        else:  # ChatGPT
            model = ChatOpenAI(model="gpt-4o-mini", openai_api_key=openai_api_key, temperature=0.1)
         # G·∫Øn prompt template v√†o chain QA
        prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
        chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)
        return chain
    except Exception as e:
        st.error(f"L·ªói t·∫°o chain: {e}")
        return None

def user_input(user_question, session_dir, answer_mode, show_reasoning, provider="Gemini"):
    try:
         # Ch·ªçn embeddings ph√π h·ª£p
        if provider == "Gemini":
            embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        else:
            embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        # Ki·ªÉm tra FAISS DB t·ªìn t·∫°i ch∆∞a
        if not os.path.exists(session_dir):
            st.error("Kh√¥ng t√¨m th·∫•y FAISS index. H√£y ph√¢n t√≠ch t√†i li·ªáu tr∆∞·ªõc." if lang == "vi" else "FAISS index not found. Please analyze document first.")
            error  = "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ƒë·ªÉ tr·∫£ l·ªùi." if lang == "vi" else "No data found to answer."
            return error

         # Load l·∫°i FAISS DB
        new_db = FAISS.load_local(session_dir, embeddings, allow_dangerous_deserialization=True)
        
        # T√¨m ki·∫øm c√°c ƒëo·∫°n vƒÉn b·∫£n g·∫ßn gi·ªëng c√¢u h·ªèi
        docs = new_db.similarity_search(user_question)
        
        # T·∫°o chain QA
        chain = get_conversational_chain(answer_mode, show_reasoning, provider)
        if not chain:
            error = "Kh√¥ng t·∫°o ƒë∆∞·ª£c chain." if lang == "vi" else "Failed to create chain."
            return error

         # G·ªçi chain ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi
        response = chain({"input_documents": docs, "question": user_question}, return_only_outputs=True)
        answer = response["output_text"]

        # L∆∞u l·ªãch s·ª≠ h·ªôi tho·∫°i
        st.session_state["chat_history"].append({
            "question": user_question,
            "answer": answer
        })
        return answer
    except Exception as e:
        error = f"L·ªói x·ª≠ l√Ω c√¢u h·ªèi: {e}" if lang == "vi" else f"Error processing question: {e}"
        st.error(error)
        return error

# ---------- Streamlit UI ----------
# C·∫•u h√¨nh app Streamlit
st.set_page_config(page_title="Chatbot RAG")

# Kh·ªüi t·∫°o tr·∫°ng th√°i ng√¥n ng·ªØ
if "language" not in st.session_state:
    st.session_state["language"] = "vi"  # Ng√¥n ng·ªØ m·∫∑c ƒë·ªãnh l√† ti·∫øng Vi·ªát
    
# T·∫°o session_id cho m·ªói l·∫ßn ch·∫°y (ƒë·ªÉ l∆∞u vectorstore ri√™ng bi·ªát)
if "session_id" not in st.session_state:
    st.session_state["session_id"] = str(uuid.uuid4())
session_dir = f"faiss_index_{st.session_state['session_id']}"

# Kh·ªüi t·∫°o l·ªãch s·ª≠ chat
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

# C·∫•u h√¨nh app Streamlit
lang = st.session_state["language"]
st.set_page_config(page_title=LANGUAGE_STRINGS[lang]["title"])
st.title(LANGUAGE_STRINGS[lang]["title"])
# Thanh sidebar: upload file + c·∫•u h√¨nh
with st.sidebar:
    st.title(LANGUAGE_STRINGS[lang]["menu"])
    st.selectbox(
        "Ng√¥n ng·ªØ / Language",
        options=["vi", "en"],
        format_func=lambda x: "Ti·∫øng Vi·ªát" if x == "vi" else "English",
        key="language"
    )
    uploaded_files = st.file_uploader(
        LANGUAGE_STRINGS[lang]["upload_label"], 
        accept_multiple_files=True,
        type=["pdf", "docx", "xlsx", "csv"]
    )
    
    st.markdown(LANGUAGE_STRINGS[lang]["chunk_size_label"])
    chunk_size = st.number_input(
        LANGUAGE_STRINGS[lang]["chunk_size_label"], 
        min_value=500, 
        max_value=20000, 
        value=10000, 
        step=500, 
        key="chunk_size"
    )
    chunk_overlap = st.number_input(
        LANGUAGE_STRINGS[lang]["chunk_overlap_label"],
        min_value=0,
        max_value=5000,
        value=1000,
        step=100,
        key="chunk_overlap"
    )
    answer_mode = st.radio(
        LANGUAGE_STRINGS[lang]["answer_mode_label"],
        LANGUAGE_STRINGS[lang]["answer_mode_options"],
        key="answer_mode"
    )
    show_reasoning = st.checkbox(
        LANGUAGE_STRINGS[lang]["show_reasoning_label"],
        key="show_reasoning"
    )
    provider = st.radio(
        LANGUAGE_STRINGS[lang]["provider_label"],
        LANGUAGE_STRINGS[lang]["provider_options"],
        key="provider"
    )
    analyze_btn = st.button(LANGUAGE_STRINGS[lang]["analyze_button"])
    clear_btn = st.button(LANGUAGE_STRINGS[lang]["clear_button"], on_click=clear_chat_history)



# Preview file upload
if uploaded_files:
    st.subheader(LANGUAGE_STRINGS[lang]["preview_header"])
    for f in uploaded_files:
        b = f.getvalue()  
        text, preview_info = get_text_from_file(b, f.name)
        show_preview_from_file(b, f.name, preview_info)

# Khi b·∫•m n√∫t Ph√¢n t√≠ch
if analyze_btn:
    if not uploaded_files:
        st.error(LANGUAGE_STRINGS[lang]["error_no_file"])
    else:
        with st.spinner("ƒêang ph√¢n t√≠ch..." if lang == "vi" else "Analyzing..."):
            all_text = ""
            for f in uploaded_files:
                b = f.getvalue()
                text, preview_info = get_text_from_file(b, f.name)
                all_text += text + "\n\n"
            if all_text.strip():
                chunks = get_text_chunks(all_text, chunk_size, chunk_overlap)
                if chunks:
                    get_vector_store(chunks, session_dir, provider)
                else:
                    st.error(LANGUAGE_STRINGS[lang]["error_no_chunks"])
            else:
                st.error(LANGUAGE_STRINGS[lang]["error_no_content"])

# QA area
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

# Hi·ªÉn th·ªã to√†n b·ªô l·ªãch s·ª≠ h·ªôi tho·∫°i
for chat in st.session_state["chat_history"]:
    with st.chat_message("user"):
        st.markdown(chat["question"])
    with st.chat_message("assistant"):
        st.markdown(chat["answer"])

# √î nh·∫≠p c√¢u h·ªèi ki·ªÉu ChatGPT
if prompt := st.chat_input(LANGUAGE_STRINGS[lang]["chat_input_placeholder"]):
    with st.chat_message("user"):
        st.markdown(prompt)

    # G·ªçi x·ª≠ l√Ω tr·∫£ l·ªùi
    answer = user_input(prompt, session_dir, answer_mode, show_reasoning, provider)
    with st.chat_message("assistant"):
        st.markdown(answer)